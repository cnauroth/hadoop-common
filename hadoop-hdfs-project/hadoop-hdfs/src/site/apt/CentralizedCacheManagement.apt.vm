~~ Licensed under the Apache License, Version 2.0 (the "License");
~~ you may not use this file except in compliance with the License.
~~ You may obtain a copy of the License at
~~
~~   http://www.apache.org/licenses/LICENSE-2.0
~~
~~ Unless required by applicable law or agreed to in writing, software
~~ distributed under the License is distributed on an "AS IS" BASIS,
~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
~~ See the License for the specific language governing permissions and
~~ limitations under the License. See accompanying LICENSE file.

  ---
  Hadoop Distributed File System-${project.version} - Centralized Cache management in HDFS
  ---
  ---
  ${maven.build.timestamp}

Centralized Cache Management in HDFS

  \[ {{{./index.html}Go Back}} \]

%{toc|section=1|fromDepth=0}

* {Background}

  Normally, HDFS relies on the operating system to cache data it reads from disk.
  However, HDFS can also be configured to use centralized cache management. Under
  centralized cache management, the HDFS NameNode itself decides which blocks
  should be cached, and where they should be cached.

  Centralized cache management has several advantages. First of all, it
  prevents frequently used block files from being evicted from memory. This is
  particularly important when the size of the working set exceeds the size of
  main memory, which is true for many big data applications. Secondly, when
  HDFS decides what should be cached, it can let clients know about this
  information through the getFileBlockLocations API. Finally, when the DataNode
  knows a block is locked into memory, it can provide access to that block via
  mmap.

* {Use Cases}

  Centralized cache management is most useful for files which are accessed very
  often. For example, a "fact table" in Hive which is often used in joins is a
  good candidate for caching. On the other hand, when running a classic
  "word count" MapReduce job which counts the number of words in each
  document, there may not be any good candidates for caching, since all the
  files may be accessed exactly once.

  * {Architecture}

  With centralized cache management, the NameNode coordinates all caching
  across the cluster. It receives cache information from each DataNode via the
  cache report, a periodic message that describes all the blocks IDs cached on
  a given DataNode. The NameNode will reply to DataNode heartbeat messages
  with commands telling it which blocks to cache and which to uncache.

  The NameNode stores a set of path cache directives, which tell it which files
  to cache. These directives are persisted to the edit log and fsimage, and
  will be loaded if the cluster is restarted.

  Periodically, the NameNode rescans the namespace, to see which blocks need to
  be cached based on the current set of path cache directives. We do not
  currently cache blocks which are under construction or otherwise incomplete.

  Caching is currently done on a per-file basis, although we would like to add
  block-level granularity in the future.

  * {Interface}

  The NameNode stores a list of "cache directives."  These directives contain a
  path as well as the number of times blocks in that path should be replicated.
  
  Paths can be either directories or files. If the path specifies a file, that
  file is cached. If the path specifies a directory, all the files in the
  directory will be cached. However, this process is not recursive-- only the
  direct children of the directory will be cached.

  Path cache directives can be created by the <<<hdfs cacheAdmin
  -addDirective>>> command and removed via the <<<hdfs cacheAdmin
  -removeDirective>>> command. To list the current path cache directives, use
  <<<hdfs cacheAdmin -listDirectives>>>. Each path cache directive has a
  unique 64-bit ID number which will not be reused if it is deleted.

  Directives are grouped into "cache pools."  Each cache pool gets a share of
  the cluster's resources. Additionally, cache pools are used for
  authentication. Cache pools have a mode, user, and group, similar to regular
  files. The same authentication rules are applied as for normal files. So, for
  example, if the mode is 0777, any user can add or remove directives from the
  cache pool. If the mode is 0644, only the owner can write to the cache pool,
  but anyone can read from it. And so forth.

  Cache pools are identified by name. They can be created by the <tt>hdfs
  cacheAdmin -addPool</tt> command, modified by the <tt>hdfs cacheAdmin
  -modifyPool</tt> command, and removed via the <tt>hdfs cacheAdmin
  -removePool</tt> command. To list the current cache pools, use <<<hdfs
  cacheAdmin -listPools>>>
  
  * {Configuration}

  In order to lock block files into memory, the DataNode relies on native JNI
  code found in <<<libhadoop.so>>>. Be sure to enable JNI if you are using
  HDFS centralized cache management.

  Also be sure to configure the following:
  
  * dfs.namenode.caching.enabled

  This must be set to true to enable caching. If this is false, the NameNode
  will ignore cache reports, and will not ask DataNodes to cache
  blocks.
  
  * dfs.datanode.max.locked.memory

  The DataNode will treat this as the maximum amount of memory it can use for
  its cache. When setting this value, please remember that you will need space
  in memory for other things, such as the Java virtual machine (JVM) itself and
  the operating system's page cache.

  If you get the error "Cannot start datanode because the configured max
  locked memory size... is more than the datanode's available RLIMIT_MEMLOCK
  ulimit," that means that the operating system is imposing a lower limit
  on the amount of memory that you can lock than what you have configured. To
  fix this, you must adjust the ulimit -l value that the DataNode runs with.
  Usually, this value is configured in <<</etc/security/limits.conf>>>.
  However, it will vary depending on what operating system and distribution you
  are using.

  You will know that you have correctly configured this value when you can run
  <<<ulimit -l>>> from the shell and get back either a higher value than what
  you have configured with <<<dfs.datanode.max.locked.memory>>>, or the string
  "ulimited," indicating that there is no limit.
